{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##Requirements","metadata":{"id":"AiTYvDPpHuM-"}},{"cell_type":"code","source":"#Libraries import\nimport os\nimport sys\nimport random\nimport math\nimport warnings\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom skimage.io import imread\nfrom matplotlib.cm import get_cmap\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage\nfrom skimage.morphology import binary_opening, disk, label\nimport gc; gc.enable() # memory is tight","metadata":{"id":"CyMIph58vzHg","execution":{"iopub.status.busy":"2023-06-21T03:13:18.416822Z","iopub.execute_input":"2023-06-21T03:13:18.417393Z","iopub.status.idle":"2023-06-21T03:13:19.213420Z","shell.execute_reply.started":"2023-06-21T03:13:18.417354Z","shell.execute_reply":"2023-06-21T03:13:19.212521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Masked RCNN repository\n!git clone https://github.com/maxw1489/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n\n# Importing Mask RCNN\nsys.path.append(os.path.join('Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","metadata":{"id":"UgqcotSfGOJF","outputId":"44c49ace-51a6-4fe9-bbe8-727c9a1bcf68","execution":{"iopub.status.busy":"2023-06-21T03:13:19.214979Z","iopub.execute_input":"2023-06-21T03:13:19.215334Z","iopub.status.idle":"2023-06-21T03:13:23.163256Z","shell.execute_reply.started":"2023-06-21T03:13:19.215299Z","shell.execute_reply":"2023-06-21T03:13:23.162245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Wrangling\n","metadata":{"id":"6p5St1WtwAtF"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"Hc7kZ1q5G2gE","outputId":"e9f6a5b6-9549-411a-e14d-fce0706bf445","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/content/drive/MyDrive/Airbus ship detection/AirbusShipDetection'\nmain_dir = '/content/working'\ntrain_dir = os.path.join(data_dir, 'train_v2')\ntest_dir = os.path.join(data_dir, 'test_v2')","metadata":{"id":"7_bFZWE98SDs","execution":{"iopub.status.busy":"2023-06-21T03:13:34.227620Z","iopub.execute_input":"2023-06-21T03:13:34.227930Z","iopub.status.idle":"2023-06-21T03:13:34.233714Z","shell.execute_reply.started":"2023-06-21T03:13:34.227903Z","shell.execute_reply":"2023-06-21T03:13:34.232695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample data/ Whole data\ndebug = False","metadata":{"id":"5_p38UkyF6CD","execution":{"iopub.status.busy":"2023-06-21T03:13:34.235499Z","iopub.execute_input":"2023-06-21T03:13:34.236412Z","iopub.status.idle":"2023-06-21T03:13:34.245549Z","shell.execute_reply.started":"2023-06-21T03:13:34.236347Z","shell.execute_reply":"2023-06-21T03:13:34.244227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg',\n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images\n\ndef filter_file_names(directory, exclude_list):\n    file_names = []\n    with os.scandir(directory) as entries:\n        for entry in entries:\n            if entry.name not in exclude_list and entry.is_file():\n                file_names.append(entry.name)\n    return file_names\n\ntrain_names = filter_file_names(train_dir, exclude_list)\ntest_names = filter_file_names(test_dir, exclude_list)\n\nprint(len(train_names), len(test_names))\n","metadata":{"id":"EKhd1Ie5V_pN","outputId":"ffe8dbd5-082e-4c75-dbc5-abb4f2d568c8","execution":{"iopub.status.busy":"2023-06-21T03:13:34.247025Z","iopub.execute_input":"2023-06-21T03:13:34.247572Z","iopub.status.idle":"2023-06-21T03:14:05.733928Z","shell.execute_reply.started":"2023-06-21T03:13:34.247540Z","shell.execute_reply":"2023-06-21T03:14:05.732936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the list to a DataFrame\ndf = pd.DataFrame({'File Name': train_names})\n\n# Write the DataFrame to a CSV file\ndf.to_csv('/content/drive/MyDrive/Airbus ship detection/train_names.csv', index=False)\n\n\n# Convert the list to a DataFrame\ndf = pd.DataFrame({'File Name': test_names})\n\n# Write the DataFrame to a CSV file\ndf.to_csv('/content/drive/MyDrive/Airbus ship detection/test_names.csv', index=False)","metadata":{"id":"vJlkh3-lb1aA","execution":{"iopub.status.busy":"2023-06-21T03:14:05.735435Z","iopub.execute_input":"2023-06-21T03:14:05.736046Z","iopub.status.idle":"2023-06-21T03:14:06.187606Z","shell.execute_reply.started":"2023-06-21T03:14:05.736009Z","shell.execute_reply":"2023-06-21T03:14:06.186652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.read_csv('/content/drive/MyDrive/Airbus ship detection/train_names.csv')\n\ntrain_names1 = df['File Name']\n\n\ndf = pd.read_csv('/content/drive/MyDrive/Airbus ship detection/test_names.csv')\n\ntest_names1 = df['File Name']\n\n","metadata":{"id":"jy-bCBOBdcLk","outputId":"c52cc4e0-f860-474a-a949-529962012411","execution":{"iopub.status.busy":"2023-06-21T03:14:06.189046Z","iopub.execute_input":"2023-06-21T03:14:06.189393Z","iopub.status.idle":"2023-06-21T03:14:06.296229Z","shell.execute_reply.started":"2023-06-21T03:14:06.189359Z","shell.execute_reply":"2023-06-21T03:14:06.295286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code to filter available images in train_ship_segmentation file\n\n# Path to the folder containing the images\nimage_folder = '/content/drive/MyDrive/Airbus ship detection/airbus-ship-detection/train_v2'\n\n# Path to the CSV file\ncsv_file = '/content/drive/MyDrive/Airbus ship detection/airbus-ship-detection/train_ship_segmentations_v2.csv'\n\n# Get the list of image names in the folder\nimage_names = [os.path.splitext(filename)[0] + '.jpg' for filename in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, filename))]\n\n# Read the CSV file\ndata = pd.read_csv(csv_file)\n\n# Filter the data based on image names\nfiltered_data = data[data['ImageId'].isin(image_names)]\n\n# Save the filtered data to a new CSV file\nfiltered_data.to_csv('/content/drive/MyDrive/Airbus ship detection/airbus-ship-detection/train_ship_segmentations_v3.csv',index=False)","metadata":{"id":"xhdK6DWRYZgT","execution":{"iopub.status.busy":"2023-06-21T03:14:06.297815Z","iopub.execute_input":"2023-06-21T03:14:06.298172Z","iopub.status.idle":"2023-06-21T03:15:12.609101Z","shell.execute_reply.started":"2023-06-21T03:14:06.298139Z","shell.execute_reply":"2023-06-21T03:15:12.607831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training dataset\nrle = data_dir + '/train_ship_segmentations_v3.csv'\nannotations = pd.read_csv(rle)\n","metadata":{"id":"UqQvvopn9Ipr","outputId":"3cbe4b9f-f11d-40b6-80f1-9c1712666400","execution":{"iopub.status.busy":"2023-06-21T03:15:12.610400Z","iopub.execute_input":"2023-06-21T03:15:12.610765Z","iopub.status.idle":"2023-06-21T03:15:13.136374Z","shell.execute_reply.started":"2023-06-21T03:15:12.610733Z","shell.execute_reply":"2023-06-21T03:15:13.135405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gutcheck images","metadata":{"id":"VCbjzCx_acGI"}},{"cell_type":"code","source":"im_names, image_annotations = train_names1, annotations\n","metadata":{"id":"A5J73Z9-aXtB","execution":{"iopub.status.busy":"2023-06-21T03:15:13.137948Z","iopub.execute_input":"2023-06-21T03:15:13.138306Z","iopub.status.idle":"2023-06-21T03:15:13.143060Z","shell.execute_reply.started":"2023-06-21T03:15:13.138273Z","shell.execute_reply":"2023-06-21T03:15:13.142023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = imread(os.path.join(train_dir, im_names[60])) # read  image from filepath\n_ = plt.imshow(ds)","metadata":{"id":"mHa6EzZBDwkP","execution":{"iopub.status.busy":"2023-06-21T03:15:13.144810Z","iopub.execute_input":"2023-06-21T03:15:13.145190Z","iopub.status.idle":"2023-06-21T03:15:13.659987Z","shell.execute_reply.started":"2023-06-21T03:15:13.145158Z","shell.execute_reply":"2023-06-21T03:15:13.659100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Original image size: 768 x 768\nORIG_SIZE = ds.shape[0]\n#ORIG_SIZE = 768","metadata":{"id":"7NZqBl13afvI","execution":{"iopub.status.busy":"2023-06-21T03:15:13.664327Z","iopub.execute_input":"2023-06-21T03:15:13.664944Z","iopub.status.idle":"2023-06-21T03:15:13.670911Z","shell.execute_reply.started":"2023-06-21T03:15:13.664907Z","shell.execute_reply":"2023-06-21T03:15:13.668358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Model Building","metadata":{"id":"mK9WeSjNbIJT"}},{"cell_type":"code","source":"#add coments for confirguration\nclass ModelConfig(Config):\n    # Give the configuration a recognizable name\n    NAME = 'Initial'\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 9\n\n    BACKBONE = 'resnet50'\n\n    NUM_CLASSES = 2  # background and ship classes\n\n    IMAGE_MIN_DIM = 384  #image shapes\n    IMAGE_MAX_DIM = 384\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64)  #different window sizes in RPN achors\n    TRAIN_ROIS_PER_IMAGE = 64  # Number of region of interest (RoI) proposals  -- Reduce the roi's\n    MAX_GT_INSTANCES = 2   # maximum number of ground truth instances that can be present in each image.\n    DETECTION_MAX_INSTANCES = 15  #maximum number of predicted instances that can be detected in each image.\n    DETECTION_MIN_CONFIDENCE = 0.95  #minimum confidence threshold\n    DETECTION_NMS_THRESHOLD = 0.0  #IoU (Intersection over Union) threshold for non-maximum suppression (NMS)\n    RUN_EAGERLY = True    #Runs eagerly a bit fast, may tradeoff acc\n\n    STEPS_PER_EPOCH = 15 if debug else 150    #number of training steps to be performed in each epoch.\n    VALIDATION_STEPS = 10 if debug else 125   #Number of validation steps\n\n    ## Initializing losses weights\n    LOSS_WEIGHTS = {\n        \"rpn_class_loss\": 30.0,\n        \"rpn_bbox_loss\": 0.8,\n        \"mrcnn_class_loss\": 6.0,\n        \"mrcnn_bbox_loss\": 1.0,\n        \"mrcnn_mask_loss\": 1.2\n    }\n\nconfig = ModelConfig()\nconfig.display()","metadata":{"id":"SQu6Jyepa6DI","outputId":"a8b30b3e-56ee-4bde-fc0d-54273745925f","execution":{"iopub.status.busy":"2023-06-21T03:15:13.672450Z","iopub.execute_input":"2023-06-21T03:15:13.673231Z","iopub.status.idle":"2023-06-21T03:15:13.685382Z","shell.execute_reply.started":"2023-06-21T03:15:13.673197Z","shell.execute_reply":"2023-06-21T03:15:13.684154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **EncodedPixels** - a list of pixels for ship segmentation in a compressed format (in run-length encoding format).\n\n EncodedPixels сonsists of pairs of values that contain a start position and a run length.\n E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\n A prediction of of \"no ship in image\" have a blank value in the EncodedPixels column.\n\n\n","metadata":{"id":"d4_LvxOgy07v"}},{"cell_type":"code","source":"\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n","metadata":{"id":"-z3W7lfIa70q","execution":{"iopub.status.busy":"2023-06-21T03:15:13.687187Z","iopub.execute_input":"2023-06-21T03:15:13.687914Z","iopub.status.idle":"2023-06-21T03:15:13.696410Z","shell.execute_reply.started":"2023-06-21T03:15:13.687879Z","shell.execute_reply":"2023-06-21T03:15:13.695445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoadDataset(utils.Dataset):\n    \"\"\"Dataset class for training our dataset.\n    \"\"\"\n\n    def __init__(self, im_names, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n\n        #to add classes\n        self.add_class('ship', 1, 'Ship')\n\n        #to add images nae & annotation\n        for i, fp in enumerate(im_names):\n            annotations = image_annotations.query('ImageId==\"' + fp + '\"')['EncodedPixels']\n            self.add_image('ship', image_id=i, path=os.path.join(train_dir, fp),\n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n\n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        image = imread(fp)\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n#         print(image_id, annotations)\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                mask[:, :, i] = rle_decode(a)\n                class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","metadata":{"id":"swUwGnZTbZOx","execution":{"iopub.status.busy":"2023-06-21T03:15:13.698136Z","iopub.execute_input":"2023-06-21T03:15:13.698629Z","iopub.status.idle":"2023-06-21T03:15:13.714281Z","shell.execute_reply.started":"2023-06-21T03:15:13.698598Z","shell.execute_reply":"2023-06-21T03:15:13.712971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split. Only considers 100 images if debug is True\nfrom sklearn.model_selection import train_test_split\ntrain_names1 = annotations[annotations.EncodedPixels.notnull()].ImageId.unique().tolist()  ## override with ships\n\ntest_size = config.VALIDATION_STEPS * config.IMAGES_PER_GPU\nim_names_train, im_names_val = train_test_split(train_names1, test_size=test_size, random_state=42)\n\nif debug:\n    im_names_train = im_names_train[:100]\n    im_names_val = im_names_val[:100]\n    test_names1 = test_names1[:100]\n\n","metadata":{"id":"aTNQT4fA9ImY","outputId":"cc5a3c96-7160-4ae1-bd40-ae62ae1d1bce","execution":{"iopub.status.busy":"2023-06-21T03:15:13.715857Z","iopub.execute_input":"2023-06-21T03:15:13.717851Z","iopub.status.idle":"2023-06-21T03:15:13.779302Z","shell.execute_reply.started":"2023-06-21T03:15:13.717654Z","shell.execute_reply":"2023-06-21T03:15:13.778172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing train and validation data usingLoadDataset class.","metadata":{"id":"xxjdLu-3K6wT"}},{"cell_type":"code","source":"%%time\n# prepare the training dataset\ndataset_train = LoadDataset(im_names_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","metadata":{"id":"21kXOwC3K0mr","outputId":"cd9c60d8-e4f4-4af1-c92c-180e4a0d8ed1","execution":{"iopub.status.busy":"2023-06-21T03:15:13.781279Z","iopub.execute_input":"2023-06-21T03:15:13.782289Z","iopub.status.idle":"2023-06-21T03:21:59.816096Z","shell.execute_reply.started":"2023-06-21T03:15:13.782255Z","shell.execute_reply":"2023-06-21T03:21:59.815105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# prepare the validation dataset\ndataset_val = LoadDataset(im_names_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"id":"zpnqiBNzK9Qy","outputId":"3968d638-04ac-4d37-e7c4-4cf8af1d2f76","execution":{"iopub.status.busy":"2023-06-21T03:21:59.817407Z","iopub.execute_input":"2023-06-21T03:21:59.818513Z","iopub.status.idle":"2023-06-21T03:22:10.527641Z","shell.execute_reply.started":"2023-06-21T03:21:59.818475Z","shell.execute_reply":"2023-06-21T03:22:10.526639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{"id":"cNKsRAouLQZi"}},{"cell_type":"code","source":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=main_dir)\n\n","metadata":{"id":"5vYXmVSlLK-6","execution":{"iopub.status.busy":"2023-06-21T04:45:48.865252Z","iopub.execute_input":"2023-06-21T04:45:48.865709Z","iopub.status.idle":"2023-06-21T04:45:55.900773Z","shell.execute_reply.started":"2023-06-21T04:45:48.865672Z","shell.execute_reply":"2023-06-21T04:45:55.899659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 0.003\n\n# Train Mask-RCNN Model\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"lRtaE_zCLT6G","execution":{"iopub.status.busy":"2023-06-21T04:45:58.789438Z","iopub.execute_input":"2023-06-21T04:45:58.789921Z","iopub.status.idle":"2023-06-21T04:45:58.795967Z","shell.execute_reply.started":"2023-06-21T04:45:58.789887Z","shell.execute_reply":"2023-06-21T04:45:58.794663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n## Only training classification, bounding box regression, and mask prediction head (ROI align)\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE*2,\n            epochs=2,\n            layers='heads')\n\nhistory = model.keras_model.history.history\n\n#losses\n#loss : Overall loss\n#rpn_class_loss: loss associated with the classification of anchor boxes in Region Proposal Network (RPN) part\n#rpn_bbox_loss: loss associated with refining the coordinates of the proposed bounding box regions by the RPN.\n#mrcnn_class_loss: loss associated with the classification of objects by the Mask R-CNN network\n#mrcnn_bbox_loss: loss associated with refining the coordinates of the bounding boxes predicted by the Mask R-CNN network.\n#mrcnn_mask_loss: loss associated with the segmentation masks predicted by the Mask R-CNN network.","metadata":{"id":"3XeELIarLg1U","outputId":"fe9f9c8b-e2a8-4a12-99a6-bd693d5b6001","execution":{"iopub.status.busy":"2023-06-21T04:46:05.243067Z","iopub.execute_input":"2023-06-21T04:46:05.243484Z","iopub.status.idle":"2023-06-21T04:58:59.703294Z","shell.execute_reply.started":"2023-06-21T04:46:05.243431Z","shell.execute_reply":"2023-06-21T04:58:59.702140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training all layers. This includes both the backbone network and the heads\n#%%time\n # LEARNING_RATE\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=4 if debug else 40,\n            layers='all')\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","metadata":{"id":"vsG_qZO_4r7x","outputId":"b8fdd79e-8869-48bb-b081-2da8c49237b4","execution":{"iopub.status.busy":"2023-06-21T08:53:25.696586Z","iopub.execute_input":"2023-06-21T08:53:25.697433Z","iopub.status.idle":"2023-06-21T08:53:52.708775Z","shell.execute_reply.started":"2023-06-21T08:53:25.697398Z","shell.execute_reply":"2023-06-21T08:53:52.706952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nsave_path = '/content/drive/MyDrive/Airbus ship detection/model_history.pkl'\n\nwith open(save_path, 'wb') as file:\n    pickle.dump(history, file)","metadata":{"id":"S752KuRHEaDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model 1's history\nepochs = range(1, len(new_history['loss'])+1)\npd.DataFrame(new_history, index=epochs)\n","metadata":{"id":"2prFmBQcmLak","outputId":"f4287852-6c78-4af0-e610-f336b6bd757c","execution":{"iopub.status.busy":"2023-06-21T08:55:19.853359Z","iopub.execute_input":"2023-06-21T08:55:19.854499Z","iopub.status.idle":"2023-06-21T08:55:19.898361Z","shell.execute_reply.started":"2023-06-21T08:55:19.854424Z","shell.execute_reply":"2023-06-21T08:55:19.897151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train vs validation plot\nplt.figure(figsize=(21,11))\n\nplt.subplot(231)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class loss\")\nplt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class loss\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\nplt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class loss\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class loss\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\nplt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\nplt.legend()\n\nplt.show()","metadata":{"id":"alS5SzHQmO-G","outputId":"71d6e58a-d75f-49ed-f571-8fc577c26b1f","execution":{"iopub.status.busy":"2023-06-21T08:53:24.822108Z","iopub.status.idle":"2023-06-21T08:53:24.822607Z","shell.execute_reply.started":"2023-06-21T08:53:24.822335Z","shell.execute_reply":"2023-06-21T08:53:24.822357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history[\"val_loss\"])\n# From the plot. Set best epoch path here epoch 33\nmodel_path = \"/content/mask_rcnn_initial_0033.h5\"\n","metadata":{"id":"sYovIdRgmT3-","execution":{"iopub.status.busy":"2023-06-21T04:35:15.520322Z","iopub.execute_input":"2023-06-21T04:35:15.521422Z","iopub.status.idle":"2023-06-21T04:35:15.526540Z","shell.execute_reply.started":"2023-06-21T04:35:15.521372Z","shell.execute_reply":"2023-06-21T04:35:15.525514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Bounding box and confidence score","metadata":{"id":"Kfa7mhE3rMeh"}},{"cell_type":"code","source":"class InferenceConfig(ModelConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference',\n                          config=inference_config,\n                          model_dir=main_dir)\n\n# Load trained weights (fill in path to trained weights here)\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","metadata":{"id":"ro_m2rv5mWLk","outputId":"2a27c44c-7dcf-44fb-e54f-9f86d9eb5027","execution":{"iopub.status.busy":"2023-06-21T04:35:51.753099Z","iopub.execute_input":"2023-06-21T04:35:51.753543Z","iopub.status.idle":"2023-06-21T04:35:56.237324Z","shell.execute_reply.started":"2023-06-21T04:35:51.753506Z","shell.execute_reply":"2023-06-21T04:35:56.236240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","metadata":{"id":"R2S1obrImasU","execution":{"iopub.status.busy":"2023-06-21T04:35:56.239256Z","iopub.execute_input":"2023-06-21T04:35:56.239621Z","iopub.status.idle":"2023-06-21T04:35:56.245378Z","shell.execute_reply.started":"2023-06-21T04:35:56.239594Z","shell.execute_reply":"2023-06-21T04:35:56.244510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show few example of ground truth vs. predictions on the validation dataset\ndataset = dataset_val\nfig = plt.figure(figsize=(10, 40))\n\nfor i in range(4,6,1):\n\n    image_id = random.choice(dataset.image_ids)\n\n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config,      #load original images along with ground truth\n                               image_id, use_mini_mask=False)\n\n# print(original_image.shape)\n    plt.subplot(8, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n\n    plt.subplot(8, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n                                dataset.class_names, r['scores'],\n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","metadata":{"id":"cI9E8jmImc5t","outputId":"97b4f977-06ab-4c29-ebb5-cd5a66ce9b69","execution":{"iopub.status.busy":"2023-06-21T03:22:20.152141Z","iopub.execute_input":"2023-06-21T03:22:20.152730Z","iopub.status.idle":"2023-06-21T03:22:29.452658Z","shell.execute_reply.started":"2023-06-21T03:22:20.152697Z","shell.execute_reply":"2023-06-21T03:22:29.451647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy metrics","metadata":{"id":"h4OQfH6XHLWV"}},{"cell_type":"markdown","source":"Map, IOU and F-2 score\n","metadata":{"id":"LXB6EEIqM7pA"}},{"cell_type":"code","source":"\nimport numpy as np\nimport warnings\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n# Define a function to calculate AP, mAP, IoU, and  F2 score\ndef calculate_ap_map_iou_f2(model, dataset, inference_config):\n    APs = []\n    IoUs = []\n    F2s = []\n    gt_bbox_list = []\n    gt_class_id_list = []\n    gt_mask_list = []\n    pred_bbox_list = []\n    pred_class_id_list = []\n    pred_score_list = []\n    \n\n    # Loop through the dataset\n    for image_id in dataset.image_ids:\n        original_image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n            modellib.load_image_gt(dataset, inference_config, image_id, use_mini_mask=False)\n\n        # Append ground truth annotations to lists\n        gt_bbox_list.append(gt_bbox)\n        gt_class_id_list.append(gt_class_id)\n        gt_mask_list.append(gt_mask)\n\n        # Run inference on the image\n        results = model.detect([original_image])\n        r = results[0]\n\n        # Append predicted results to lists\n        pred_bbox = r['rois']\n        pred_class_ids = r['class_ids']\n        pred_scores = r['scores']\n        pred_masks = r['masks']\n        pred_bbox_list.append(pred_bbox)\n        pred_class_id_list.append(pred_class_ids)\n        pred_score_list.append(pred_scores)\n\n        # Compute AP and IoU for the image\n        AP, _, _, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask, pred_bbox, pred_class_ids, pred_scores, pred_masks, iou_threshold=0.5)  # Use IoU threshold of 0.5 for competition evaluation\n        APs.append(AP)\n\n        # Calculate IoU\n        valid_overlaps = overlaps[~np.isnan(overlaps)]  # Exclude NaN IoU values\n        if len(valid_overlaps) > 0:\n            IoU = np.mean(valid_overlaps)\n            IoUs.append(IoU)\n\n        # Calculate  F2 score\n        TP = len(overlaps) - np.isnan(overlaps).sum()  # True Positives (excluding NaN IoU values)\n        FN = len(gt_bbox) - TP  # False Negatives\n        FP = len(pred_bbox) - TP  # False Positives\n        beta = 2  # F2 score weight for recall (increase weight of recall)\n        F2 = ((1 + beta**2) * TP) / ((1 + beta**2) * TP + beta**2 * FN + FP)  # Modified Kaggle F2 score formula\n        #F2s.append(F2)\n        if F2 >= 1:  # Only consider F1 scores >= 1\n            F2s.append(F2)\n\n    # Calculate mean Average Precision (mAP), mean IoU (excluding NaN values), and mean  F2 score\n    mAP = np.mean(APs)\n    mean_IoU = np.nanmean(IoUs) if len(IoUs) > 0 else np.nan\n    mean_F2 = np.mean(F2s)\n\n    return APs, mAP, IoUs, mean_IoU, F2s, mean_F2\n\n# Call the function to calculate AP, mAP, IoU, and  F2 score\nAPs, mAP, IoUs, mean_IoU, F2s, mean_F2 = calculate_ap_map_iou_f2(model, dataset_val, inference_config)\n\n# Print the results\nprint(\"Average Precision (AP) per class:\")\n# for i, AP in enumerate(APs):\n#     print(f\"Class {i}: AP = {AP}\")\n\nprint(\"Mean Average Precision (mAP):\", mAP)\n\n# print(\"Intersection over Union (IoU) per image:\")\n# for i, IoU in enumerate(IoUs):\n#     print(f\"Image {i}: IoU = {IoU}\")\n\nprint(\"Mean IoU :\", mean_IoU)\n\n# print(\" F2 score per image:\")\n# for i, F2 in enumerate(F2s):\n#     print(f\"Image {i}: F2 score = {F2}\")\n\nprint(\"Mean F2 score:\", mean_F2)\n","metadata":{"id":"f0cdJtL4mly6","outputId":"84e68111-c095-4ade-f712-3fdd4ae15e1c","execution":{"iopub.status.busy":"2023-06-21T04:36:08.798644Z","iopub.execute_input":"2023-06-21T04:36:08.799015Z","iopub.status.idle":"2023-06-21T04:39:04.647357Z","shell.execute_reply.started":"2023-06-21T04:36:08.798986Z","shell.execute_reply":"2023-06-21T04:39:04.646370Z"},"trusted":true},"execution_count":null,"outputs":[]}]}